---
layout: post
title: The Q&A model using CNN
date: 2017-09-21
category: Machine Learning
tags: [Machine Learning, Algorithm]
description: Using a model based on CNN to build a Q&A system
---

# Introduction

This was the recent task given by the leader, to development a system for Q&A using the database our company's daily business. I refered the 
system provided by the IBM Waltson's papaer 

APPLYING DEEP LEARNING TO ANSWER SELECTION: A STUDY AND AN OPEN TASK.

In this paper, a system based on CNN was published and really useful, my whole work were just based on that framework.

# Training process and the system's structure 

During training, for each training question Q there is a positive answer A+(the ground truth). A training instance is constructed by pairing this A+ with a negative answer A−(a wrong answer) sampled from the whole answer space. The deep learning framework generates vector representations for the question and the two candidates:VQ , VA+ and VA− . The cosine similarities cos(VQ , VA+) and cos(VQ , VA−) are calculated and the distance between the two similarities is compared to a margin: cos(VQ , VA+ )−cos(VQ , VA−) < m . m is the margin. When this condition is satisfied, the implication is that the vector space embedding either ranks the positive answer below the negative answer, or does not sufficiently rank the positive answer above the negative answer. If cos(VQ , VA+ ) − cos(VQ , VA− ) >= m there is no update to the parameters and a new negative example is sampled until the margin is less than m (to reduce running time we set maximum 50 times in this paper). The hinge loss function is hence defined as follows:

L = max {0, m − cos(VQ, VA+ ) + cos(VQ, VA− )}

For testing, we calculate the cos(VQ,Vcandidate) between the question Q and each answer candidate Vcandidate in the pool (size 500). The candidate answer with largest cosine similarity is selected.

The two kinds of architecture:


![0](https://raw.githubusercontent.com/BigExcavator/coldsrh233.github.io/master/_posts/image/%23CnnQA/0.jpg)

The above one is the structure of Architecture I. Q is the input question provided as input to the first hidden layer HLQ. The hidden layer (HL) is defined as z = tanh(Wx+B). W is the weight matrix; B is the bias vector; x is input; z is the output of the activation function tanh. The output then flows to the CNN layer CNNQ, applied to extract question side features. P is the MaxPooling layer (we always use 1-MaxPooling in this paper) and T is the tanh layer. Similar to the question side, the answer A is processed by HLA and then features are extracted by CNNA . 1-MaxPooling P and tanh layer T will function in the last step. 

![1](https://raw.githubusercontent.com/BigExcavator/coldsrh233.github.io/master/_posts/image/%23CnnQA/1.jpg)

The above one is the structure of Architecture II. The main difference compared to Architecture I is that both question and answer sides share the same HL and CNN weights.

# The results and test

![2](https://raw.githubusercontent.com/BigExcavator/coldsrh233.github.io/master/_posts/image/%23CnnQA/2.jpg)
![3](https://raw.githubusercontent.com/BigExcavator/coldsrh233.github.io/master/_posts/image/%23CnnQA/3.jpg)

That's the result given by the paper. As we can see, the structrue that share the CNN layer has a much better performance.

In our own business. In our answer library for each question, we have one best answer, several relevant answers, others are negetive answers. The aim is to find both the rate for best asnwer and relevant answer. And the result shows that 50.7% of the selected answer are best answer and 91.2% of them are relevant answer.

The following are some screenshot of results:

![4](https://raw.githubusercontent.com/BigExcavator/coldsrh233.github.io/master/_posts/image/%23CnnQA/4.jpg)

![5](https://raw.githubusercontent.com/BigExcavator/coldsrh233.github.io/master/_posts/image/%23CnnQA/5.jpg)

# The technology used in the project

Answer Selection, Question Answering, Convolutional Neural Network (CNN), Deep Learning, Word Embedding





