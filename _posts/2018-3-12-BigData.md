---
layout: post
published: true
title: Big Data practice projects
category: Big Data
tags: [Big Data, Algorithm]
---

# Introduction

Recently, I have completed a site of small projects that practicing the using of Hadoop & Spark. Including Two-phase approach, Frequency counting using SON algorithm, Finding similar users using LSH, Decomposing Matrix to lower rank, Cluster algorithm based on HAC and K-means.

# Two-phase approach

This is a Hadoop MapReduce program, TwoPhase.java, that computes the multiplication of two given matrices using the two- phase approach.

The following are the main code of the project:

     // mapper for processing entries of matrix A
    public static class PhaseOneMapperA extends Mapper<LongWritable, Text, Text, Text> {
	
        private Text outKey = new Text();
        private Text outVal = new Text();

        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            
            String[] lines = value.toString().split("\n");
            for (String s: lines) {
                String temp[] = s.split(",");
                context.write(new Text(temp[1]), new Text(new String("A," + temp[0] + "," + temp[2])));
            }
        }
    }

    //the reduce operation
    public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            
            ArrayList<String[]> alist = new ArrayList<>();
            ArrayList<String[]> blist = new ArrayList<>();
            for (Text txt: values) {
                String[] temp1 = txt.toString().split(",");
                if (temp1[0].equals("A")) {
                    alist.add(temp1);
                }
                if (temp1[0].equals("B")) {
                    blist.add(temp1);
                }
            }
            
            for (String[] s1: alist) {
                for (String[] s2: blist) {
                    context.write(new Text(new String(s1[1] + "," + s2[1])), new Text(String.valueOf(Integer.parseInt(s1[2]) * Integer.parseInt(s2[2]))));
                }
            }

# Frequency counting using SON algorithm

Given a set of baskets, SON algorithm divides them into chunks/partitions and then proceed in two stages. First, local frequent itemsets are collected, which form candidates; next, it makes second pass through data to determine which candidates are globally frequent.  

The following code is the main part of the project:

		// produce frequent single items
		for each_basket in all_baskets: # v is each basket in string
			//split each line by comma, and count freq of each item
			each_basket = each_basket.split(',') //each_basket is a list, represents a basket, contains items

			for i in each_basket: //i is each item in a basket
				if i not in counts:
					counts[i] = 1
				else:
					counts[i] = counts[i] + 1
			num_baskets += 1

		threshold = num_baskets * support_ratio

		//- filter out items freq lower than support ratio 
		for i,j in counts.items():
			if j < threshold:
				del counts[i]

		//- get a list of frequent single items
		freq_tuples = counts.keys()
		for i in freq_tuples:
			outcome.append((i, 1))
		freq_singles = freq_tuples
		// single part over
		// after single part, frequent single items, threshold is finalized

		// - calculate the global threshold
		num_baskets_all = source.mapPartitions(lambda x: [len(list(x))]).reduce(lambda x,y: x + y)
		global_threshold = num_baskets_all * support_ratio
		phase2Out = source.mapPartitions(func2).reduceByKey(lambda x,y: x + y).filter(lambda x: x[1] >= global_threshold).map(lambda x: x[0]).collect()

# Finding similar users using LSH

In this project, I solve the problem of using LSH to find similar users, based on the fraction of the movies they have watched in common.

The main code of the projects are shown as following:


	//lsh
    def func(x):
        usersAndCand = {}
        for (a,b) in x: 
            for user in b:
                if user in usersAndCand:
                    usersAndCand[user] |= set([j for j in b if j != user])
                else:
                    usersAndCand[user] = set([j for j in b if j != user])

        return usersAndCand.items()

    def top5lrg(alist): # use selection sort 5 times
        for i in range(min(5, len(alist))):
            lrgind = i
            for k in range(i + 1, len(alist)):
                if alist[k][1] > alist[lrgind][1]:
                    lrgind = k
                elif alist[k][1] == alist[lrgind][1]:
                    if int(alist[k][0][1:]) < int(alist[lrgind][0][1:]):
                        lrgind = k
            tmp = alist[i]
            alist[i] = alist[lrgind]
            alist[lrgind] = tmp

        return sorted(alist[:5], key = lambda x: int(x[0][1:]))
        
    def tagging(x):
        ans = list()
        counter = 0
        for i in x[1]:
            ans.append((str(i) + '_' + str(counter), x[0]))
            counter += 1
        return ans

    // make the section correspond
    temp = sc.parallelize(usersAndSigs.items(),4).map(lambda x: (x[0], [x[1][i:i+4] for i in range(0, len(x[1]), 4)]))
    // print temp.collect()[:2]
    // here need to creat a func to make sure corresponds
    temp2 = temp.flatMap(tagging).groupByKey().map(lambda x: (x[0], list(x[1]))).filter(lambda x: len(x[1]) > 1).mapPartitions(func).reduceByKey(lambda x,y: list(set(x) | set(y)))
    temp3 = temp2.map(lambda x: (x[0], [(i, float(len(set(usersAndMoviesDict[x[0]]) & set(usersAndMoviesDict[i]))) / float(len(set(usersAndMoviesDict[x[0]]) | set(usersAndMoviesDict[i])))) for i in x[1]])).mapValues(top5lrg).map(lambda x: (x[0], [i[0] for i in x[1]]))

    result = temp3.collect()
    result = sorted(result, key = lambda x: int(x[0][1:]))

# Decomposing Matrix to lower rank

The latent factor modeling of utility matrix M (e.g., a rating matrix where rows represent users and columns products such as movies). In latent factor modeling, the goal is to decompose M into lower-rank matrices U and V such that the difference between M and UV is minimized. 

The main code is shown here:

	def rmse(R, ms, us):
    	diff = R - ms * us.T
    	return np.sqrt(np.sum(np.power(diff, 2)) / (M * U))


	def update(i, mat, ratings):
    	uu = mat.shape[0]
    	ff = mat.shape[1]

    	XtX = mat.T * mat
    	Xty = mat.T * ratings[i, :].T

    	for j in range(ff):
        	XtX[j, j] += LAMBDA * uu

    	return np.linalg.solve(XtX, Xty)

	# initialize target matrix
	mat = np.zeros(shape=(n, m))
	for i in lines:
		tmp = i.split(',')
		mat[int(tmp[0]) - 1][int(tmp[1]) - 1] = int(tmp[2])

	// print mat
	// initialize U, V
	U = np.ones(shape=(n, f))
	V = np.ones(shape=(f, m))

	// UV decomp
	for z in range(it):
		// training U
		for r in range(n): // r is row index of U
			for s in range(f): // s is col index of U
				tmpsum = 0
				tmpd = 0
				for j in range(m):
					tmpsum2 = 0
					if mat[r][j] > 0:
						for k in range(f):
							if k != s:
								tmpsum2 += U[r][k] * V[k][j]
						tmpsum += V[s][j] * (mat[r][j] - tmpsum2)
						tmpd += V[s][j] * V[s][j]
				U[r][s] = float(tmpsum) / tmpd
				// print 'U(' + str(r+1) + ',' + str(s+1) + ') = ' + str(U[r][s])

		for s in range(m):
			for r in range(f):
				tmpsum = 0
				tmpd = 0
				for i in range(n):
					tmpsum2 = 0
					if mat[i][s] > 0:
						for k in range(f):
							if k != r:
								tmpsum2 += U[i][k] * V[k][s]
						tmpsum += U[i][r] * (mat[i][s] - tmpsum2)
						tmpd += U[i][r] * U[i][r]
				V[r][s] = float(tmpsum) / tmpd
				// print 'V(' + str(r+1) + ',' + str(s+1) + ') = ' + str(V[r][s])
		
		// print cost
		// print V
		approx = np.dot(U, V)
		// print approx
		error = 0
		cnt = 0
		for i in range(n):
			for j in range(m):
				if mat[i][j] > 0:
					error += (mat[i][j] - approx[i][j]) * (mat[i][j] - approx[i][j])
					cnt += 1
		print cnt
		error /= float(cnt)
		error = math.sqrt(error)
		print "%.4f" %error

# Cluster algorithm based on HAC and K-means

The first is hierarchical (agglomerative/bottom-up) clustering HAC (in Python), and the second is (spherical) k-means (in Spark). K-means on unit vectors is often called spherical k-means.

The following shows the main code:

	// cosine similarity of two csc represented array
	def cos_sim(a, b):
    	return float(a.multiply(b).sum()) / (np.linalg.norm(a.toarray()) * np.linalg.norm(b.toarray()))



	def parseVector(line):
    return np.array([float(x) for x in line.split(' ')])

	// usage: closestPoint(p, kPoints)
	def closestPoint(p, centers):
    	bestIndex = 0
    	closest = float("+inf")
    	for i in range(len(centers)):
        	tempDist = cos_sim(p, centers[i])
        	// tempDist = np.sum((p - centers[i]) ** 2)
        	if tempDist < closest:
            	closest = tempDist
            	bestIndex = i
    	return bestIndex
